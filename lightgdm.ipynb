{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import xgboost as xgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# import optuna\n",
    "# from scipy.stats import randint\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/mushroom.csv')\n",
    "df2 = pd.read_csv('./datasets/mushrooms.csv')\n",
    "\n",
    "# rename columns\n",
    "df2.rename(columns={'bruises':'ruises'}, inplace=True)\n",
    "\n",
    "# df = pd.concat([df, df2], axis=0)\n",
    "# df = pd.concat([df, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25986, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>ruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>stalk-root</th>\n",
       "      <th>stalk-surface-above-ring</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>g</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>g</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>?</td>\n",
       "      <td>s</td>\n",
       "      <td>k</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class cap-shape cap-surface cap-color ruises odor gill-attachment  \\\n",
       "0     e         x           f         n      f    n               f   \n",
       "1     p       NaN           y         g      t  NaN               f   \n",
       "2     e         b           y         n      t    n               f   \n",
       "3     e         x           g         g      t    n               f   \n",
       "4     e       NaN           f       NaN      t    n               a   \n",
       "\n",
       "  gill-spacing gill-size gill-color stalk-shape stalk-root  \\\n",
       "0            w         n          b           t        NaN   \n",
       "1            c         b          k           e          b   \n",
       "2            c       NaN          n           t          r   \n",
       "3            w         b          n           t          b   \n",
       "4            w         n          n           e          ?   \n",
       "\n",
       "  stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring  \\\n",
       "0                        s                        y                      w   \n",
       "1                        f                        s                      n   \n",
       "2                        s                        s                      p   \n",
       "3                        s                        s                      p   \n",
       "4                        s                        k                    NaN   \n",
       "\n",
       "  stalk-color-below-ring veil-type veil-color ring-number ring-type  \\\n",
       "0                      p       NaN          n           o         p   \n",
       "1                      c         p          w           n         e   \n",
       "2                    NaN         p          w           o         p   \n",
       "3                    NaN         p          w           n         n   \n",
       "4                      w         p          w         NaN         l   \n",
       "\n",
       "  spore-print-color population habitat  \n",
       "0                 w          v     NaN  \n",
       "1               NaN          y       g  \n",
       "2                 b          y       w  \n",
       "3               NaN        NaN       d  \n",
       "4                 w          v       d  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(data.describe())\n",
    "\n",
    "# 统计所有行列中 NaN的个数的分布\n",
    "distri = {}\n",
    "for row in df.iterrows():\n",
    "    cnt = 0\n",
    "    for cell in row[1]:\n",
    "        if str(cell) == 'nan':\n",
    "            cnt += 1\n",
    "\n",
    "    if cnt not in distri:\n",
    "        distri[cnt] = 1\n",
    "    distri[cnt] += 1\n",
    "\n",
    "\n",
    "# 画图\n",
    "sns.barplot(x=list(distri.keys()), y=list(distri.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.04440852766874471, 0.18675440621873315, 0.4143769722158085, 0.6585469098745478, 0.8376818286769798, 0.9369275763872854, 0.9790271684753328, 0.9938428384514738, 0.9989609789886862, 1.0001154467790347]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算缺失值的比例\n",
    "ratio = {}\n",
    "for key in distri.keys():\n",
    "    ratio[key] = distri[key] / df.shape[0]\n",
    "\n",
    "sum_list = []\n",
    "for i in range(0, len(ratio)):\n",
    "    s = 0\n",
    "    # 加上比i小的所有比例\n",
    "    for j in range(0, i):\n",
    "        s += ratio[j]\n",
    "    sum_list.append(s)\n",
    "\n",
    "# 画折线图\n",
    "sns.lineplot(x=range(0, len(ratio) ), y=pd.Series(sum_list))\n",
    "print(sum_list)\n",
    "\n",
    "\n",
    "# find the last 97%\n",
    "i = 0\n",
    "while sum_list[i] < 0.97:\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃缺失值个数超过i 的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 丢弃缺失值个数超过i 的行\n",
    "# for row in df.iterrows():\n",
    "#     cnt = 0\n",
    "#     for cell in row[1]:\n",
    "#         if str(cell) == 'nan':\n",
    "#             cnt += 1\n",
    "#     if cnt > i:\n",
    "#         df.drop(row[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76697\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum().sum())\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃 veil-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['veil-type'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码插值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 用众数填充缺失值\n",
    "# df = df.apply(lambda x: x.fillna(df.mode().iloc[0]))\n",
    "\n",
    "# x = df.drop('class', axis=1)\n",
    "# y = df['class']\n",
    "\n",
    "# x = OneHotEncoder().fit_transform(x)\n",
    "# y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "mask = df.isnull()\n",
    "# class 使用 OneHotEncoder\n",
    "df['class'] = OneHotEncoder().fit_transform(df['class'].values.reshape(-1, 1)).toarray()\n",
    "# 其余使用 LabelEncoder 编码\n",
    "for col in df.columns:\n",
    "    if col != 'class':\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "df.where(~mask, np.nan, inplace=True)\n",
    "\n",
    "# KNN Imputer\n",
    "\n",
    "df = pd.DataFrame(KNNImputer(\n",
    "    n_neighbors=5).fit_transform(df), columns=df.columns)\n",
    "\n",
    "df.head()\n",
    "\n",
    "x = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正态分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in df.columns:\n",
    "#     Q1 = df[feature].quantile(0.25)\n",
    "#     Q3 = df[feature].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "\n",
    "# def outlier_treatment(colume_name) -> None:\n",
    "#     global df\n",
    "#     Q1, Q3 = df[colume_name].quantile([0.25, 0.75])\n",
    "#     IQR = Q3 - Q1\n",
    "    \n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "#     outliers_iqr = df[(df[colume_name] < lower_bound) | (df[colume_name] > upper_bound)]\n",
    "    \n",
    "#     if outliers_iqr.empty:\n",
    "#         print(f'No outliers found in {colume_name}')\n",
    "#     else:\n",
    "#         # drop\n",
    "#         print(f'Outliers found in {colume_name}, dropping {outliers_iqr.shape[0]} rows')\n",
    "#         df.drop(outliers_iqr.index, inplace=True)\n",
    "#         df = df.reset_index(drop = True)\n",
    "\n",
    "# for feature in df.columns:\n",
    "#     outlier_treatment(feature)\n",
    "\n",
    "# # ret = process_map(outlier_treatment, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20788, 21) (20788,)\n",
      "(5198, 21) (5198,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = [\n",
    "    \n",
    "]\n",
    "# {\n",
    "#     'model': 'name',\n",
    "#     'result': 'list'\n",
    "# }\n",
    "\n",
    "# f1 = make_scorer(f1_score, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = SVC()\n",
    "# svm_model.fit(x_train, y_train)\n",
    "\n",
    "# svm_pred = svm_model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'SVM',\n",
    "#     'result': svm_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(y_test, svm_pred)\n",
    "# plt.show()\n",
    "\n",
    "# print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 贝叶斯优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "#     gamma = trial.suggest_loguniform('gamma', 1e-3, 1e-1)\n",
    "#     kernel = trial.suggest_categorical('kernel', ['rbf'])\n",
    "\n",
    "#     svc = SVC(C=C, gamma=gamma, kernel=kernel)\n",
    "    \n",
    "#     # f1_score\n",
    "#     score = cross_val_score(svc, x_train, y_train, cv=5, scoring=f1).mean()\n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.optimize(objective, n_trials=96, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # svm_model = SVC(C=study.best_params['C'], gamma=study.best_params['gamma'], kernel=study.best_params['kernel'])\n",
    "# best = {'C': 0.17284149315658728, 'gamma': 0.0015771661832327244, 'kernel': 'rbf'}\n",
    "# svm_model = SVC(C=best['C'], gamma=best['gamma'], kernel=best['kernel'])\n",
    "# svm_model.fit(x_train, y_train)\n",
    "\n",
    "# svm_pred = svm_model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'SVM',\n",
    "#     'result': svm_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(y_test, svm_pred)\n",
    "# plt.show()\n",
    "\n",
    "# print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# parameters = {\n",
    "#     'n_estimators': np.arange(50, 500, 50),\n",
    "#     'max_depth': np.arange(1, 10, 1),\n",
    "#     'learning_rate': np.logspace(-3, 2, 10)\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(gb_model, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb_model = GradientBoostingClassifier(**grid_search.best_params_)\n",
    "# gb_model.fit(x_train, y_train)\n",
    " \n",
    "# gb_pred = gb_model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'GradientBoosting',\n",
    "#     'result': gb_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(y_test, gb_pred)\n",
    "# plt.show()\n",
    "\n",
    "# print(classification_report(y_test, gb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ada_model = AdaBoostClassifier()\n",
    "\n",
    "# # parameters = {\n",
    "# #     'n_estimators': np.arange(50, 500, 20),\n",
    "# #     'learning_rate': np.logspace(-3, 2, 20),\n",
    "# #     'algorithm': ['SAMME']\n",
    "# # }\n",
    "\n",
    "# # grid_search = GridSearchCV(ada_model, parameters, n_jobs=-1, cv=5, verbose=1)\n",
    "# # grid_search.fit(x_train, y_train)\n",
    "\n",
    "# # ada_model = AdaBoostClassifier(n_estimators=grid_search.best_params_[\n",
    "# #                                'n_estimators'], learning_rate=grid_search.best_params_['learning_rate'])\n",
    "\n",
    "# best_params = {'algorithm': 'SAMME',\n",
    "#                'learning_rate': 0.774263682681127, 'n_estimators': 150}\n",
    "\n",
    "# ada_model = AdaBoostClassifier(\n",
    "#     n_estimators=best_params['n_estimators'], learning_rate=best_params['learning_rate'], algorithm=best_params['algorithm'])\n",
    "\n",
    "# ada_model.fit(x_train, y_train)\n",
    "\n",
    "# ada_pred = ada_model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'AdaBoost',\n",
    "#     'result': ada_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# import time\n",
    "# joblib.dump(ada_model, f'./results/ada_model_nodrop_{time.strftime(\"%Y-%m-%d-%H-%M-%S\")}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(y_test, ada_pred)\n",
    "# plt.show()\n",
    "\n",
    "# print(classification_report(y_test, ada_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 长短期记忆网络 \n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, LSTM\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(128, input_shape=(x_train.shape[1], 1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # metrics f1_score\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with test\n",
    "# model.fit(x_train, y_train, epochs=100, batch_size=256, validation_data=(x_test, y_test))\n",
    "\n",
    "# lstm_pred = model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'LSTM',\n",
    "#     'result': lstm_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN\n",
    "\n",
    "# from keras.layers import SimpleRNN\n",
    "# from keras.models import *\n",
    "# from keras.layers import *\n",
    "\n",
    "# rnn_model = Sequential()\n",
    "\n",
    "# rnn_model.add(SimpleRNN(128, input_shape=(x_train.shape[1], 1)))\n",
    "# rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model.fit(x_train, y_train, epochs=100, batch_size=256, validation_data=(x_test, y_test))\n",
    "\n",
    "# rnn_pred = rnn_model.predict(x_test)\n",
    "\n",
    "# model_result.append({\n",
    "#     'model': 'RNN',\n",
    "#     'result': rnn_pred\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.552290\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.69      0.71      2325\n",
      "         1.0       0.76      0.79      0.78      2873\n",
      "\n",
      "    accuracy                           0.75      5198\n",
      "   macro avg       0.75      0.74      0.75      5198\n",
      "weighted avg       0.75      0.75      0.75      5198\n",
      "\n",
      "0.7621910487641951\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 设置LightGBM参数\n",
    "params = {'lambda_l1': 3.4482046572642613e-07, 'lambda_l2': 6.557494099284118e-06, 'num_leaves': 6,\n",
    "          'feature_fraction': 0.9298593243367005, 'bagging_fraction': 0.6392974057353847, 'bagging_freq': 3}\n",
    "\n",
    "# 训练模型\n",
    "bst = lgb.train(params, train_data, num_boost_round=2000,\n",
    "                valid_sets=[test_data])\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# 二值化预测结果\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(precision_score(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:03,227] A new study created in memory with name: no-name-1b84cc53-45c9-400b-9d05-0ed9d7a5345c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:07,453] Trial 0 finished with value: 0.7821765209940017 and parameters: {'lambda_l1': 0.3602330233147792, 'lambda_l2': 1.2686984609361715e-07, 'num_leaves': 126, 'feature_fraction': 0.772380593164173, 'bagging_fraction': 0.9017759432305135, 'bagging_freq': 7}. Best is trial 0 with value: 0.7821765209940017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:12,415] Trial 1 finished with value: 0.7793233728318736 and parameters: {'lambda_l1': 8.797369941670623e-08, 'lambda_l2': 4.264177402397175e-06, 'num_leaves': 39, 'feature_fraction': 0.8941139333515628, 'bagging_fraction': 0.9665429117744112, 'bagging_freq': 5}. Best is trial 0 with value: 0.7821765209940017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:17,239] Trial 2 finished with value: 0.7757762909589981 and parameters: {'lambda_l1': 0.008730380150846921, 'lambda_l2': 2.7691237292796256, 'num_leaves': 107, 'feature_fraction': 0.7364323377534521, 'bagging_fraction': 0.46570890405602994, 'bagging_freq': 5}. Best is trial 0 with value: 0.7821765209940017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:51,202] Trial 3 finished with value: 0.7796204479398188 and parameters: {'lambda_l1': 1.1834295684625669e-06, 'lambda_l2': 0.005294628166483138, 'num_leaves': 252, 'feature_fraction': 0.7621091078657607, 'bagging_fraction': 0.8447573003543204, 'bagging_freq': 6}. Best is trial 0 with value: 0.7821765209940017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:49:52,216] Trial 4 finished with value: 0.7829205807002563 and parameters: {'lambda_l1': 6.58108774882649e-06, 'lambda_l2': 2.2583944433009368e-05, 'num_leaves': 9, 'feature_fraction': 0.9560395471888771, 'bagging_fraction': 0.6465383772211833, 'bagging_freq': 3}. Best is trial 4 with value: 0.7829205807002563.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:50:01,225] Trial 5 finished with value: 0.7831881086622245 and parameters: {'lambda_l1': 2.0934316226006154e-07, 'lambda_l2': 4.352285305737986e-07, 'num_leaves': 17, 'feature_fraction': 0.7644667837181798, 'bagging_fraction': 0.9948712404412705, 'bagging_freq': 6}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:50:11,491] Trial 6 finished with value: 0.7773995915588836 and parameters: {'lambda_l1': 1.4367171701495487e-08, 'lambda_l2': 0.016499530835771346, 'num_leaves': 172, 'feature_fraction': 0.4452519535345722, 'bagging_fraction': 0.8623501327620557, 'bagging_freq': 5}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:50:41,126] Trial 7 finished with value: 0.7785996903492173 and parameters: {'lambda_l1': 0.019593008400691714, 'lambda_l2': 6.5305130216132214, 'num_leaves': 198, 'feature_fraction': 0.6833803986930795, 'bagging_fraction': 0.6678550984090019, 'bagging_freq': 3}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:05,545] Trial 8 finished with value: 0.7752309271296614 and parameters: {'lambda_l1': 0.00039903196927950084, 'lambda_l2': 0.00894728361609755, 'num_leaves': 198, 'feature_fraction': 0.4167743333453212, 'bagging_fraction': 0.8054359897780072, 'bagging_freq': 4}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:08,223] Trial 9 finished with value: 0.7789941840574751 and parameters: {'lambda_l1': 7.34551046130922e-06, 'lambda_l2': 0.00030895349925752036, 'num_leaves': 48, 'feature_fraction': 0.5745714758750772, 'bagging_fraction': 0.49157546870753543, 'bagging_freq': 3}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:34,716] Trial 10 finished with value: 0.7816406918992977 and parameters: {'lambda_l1': 0.00014009738245197625, 'lambda_l2': 1.0657652629541758e-08, 'num_leaves': 81, 'feature_fraction': 0.6223348283804455, 'bagging_fraction': 0.7470607929022159, 'bagging_freq': 1}. Best is trial 5 with value: 0.7831881086622245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:35,763] Trial 11 finished with value: 0.7858967807869188 and parameters: {'lambda_l1': 1.8458088137239206e-06, 'lambda_l2': 9.251826842882609e-06, 'num_leaves': 8, 'feature_fraction': 0.9426425941101728, 'bagging_fraction': 0.6104320766695679, 'bagging_freq': 1}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:38,057] Trial 12 finished with value: 0.7848966342046814 and parameters: {'lambda_l1': 2.4861523253852033e-07, 'lambda_l2': 1.0541449362822703e-06, 'num_leaves': 15, 'feature_fraction': 0.857693925229214, 'bagging_fraction': 0.5636691501144313, 'bagging_freq': 1}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:43,749] Trial 13 finished with value: 0.7768225584594224 and parameters: {'lambda_l1': 6.470093244705857e-05, 'lambda_l2': 3.6015440095263434e-05, 'num_leaves': 71, 'feature_fraction': 0.8707356353361102, 'bagging_fraction': 0.5730663676784941, 'bagging_freq': 1}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:51:44,607] Trial 14 finished with value: 0.7832001360312871 and parameters: {'lambda_l1': 1.2265953797707903e-08, 'lambda_l2': 6.607105618607518e-07, 'num_leaves': 5, 'feature_fraction': 0.9871282913296264, 'bagging_fraction': 0.5427446409749462, 'bagging_freq': 2}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:52:02,647] Trial 15 finished with value: 0.77863116572794 and parameters: {'lambda_l1': 2.6728287194708514e-06, 'lambda_l2': 1.0284227637239189e-08, 'num_leaves': 56, 'feature_fraction': 0.8609500858914378, 'bagging_fraction': 0.5988545884677857, 'bagging_freq': 1}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:52:10,334] Trial 16 finished with value: 0.7766856354543886 and parameters: {'lambda_l1': 8.584054470959488, 'lambda_l2': 0.0001001803118236565, 'num_leaves': 102, 'feature_fraction': 0.9257459404940401, 'bagging_fraction': 0.42084949048927933, 'bagging_freq': 2}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:52:11,992] Trial 17 finished with value: 0.7797945205479452 and parameters: {'lambda_l1': 2.3340315128291425e-07, 'lambda_l2': 6.4125323362191575e-06, 'num_leaves': 27, 'feature_fraction': 0.8284855231687334, 'bagging_fraction': 0.737895576853778, 'bagging_freq': 2}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:52:33,064] Trial 18 finished with value: 0.7754681326232606 and parameters: {'lambda_l1': 0.0015192385416000443, 'lambda_l2': 0.0016953874487548014, 'num_leaves': 159, 'feature_fraction': 0.9968285594819896, 'bagging_fraction': 0.6145340741846951, 'bagging_freq': 1}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:52:57,115] Trial 19 finished with value: 0.7778735136998104 and parameters: {'lambda_l1': 3.131930250327496e-05, 'lambda_l2': 0.2787052231301742, 'num_leaves': 76, 'feature_fraction': 0.8213500009560317, 'bagging_fraction': 0.5202099605808419, 'bagging_freq': 2}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:02,840] Trial 20 finished with value: 0.7828913601368691 and parameters: {'lambda_l1': 7.131280828012655e-07, 'lambda_l2': 1.1027547335433545e-07, 'num_leaves': 36, 'feature_fraction': 0.9251522583619993, 'bagging_fraction': 0.7208613967916795, 'bagging_freq': 4}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:06,912] Trial 21 finished with value: 0.7824303537856777 and parameters: {'lambda_l1': 1.313916943791495e-08, 'lambda_l2': 1.2227615701683678e-06, 'num_leaves': 5, 'feature_fraction': 0.9858174691787568, 'bagging_fraction': 0.5481976871335422, 'bagging_freq': 2}. Best is trial 11 with value: 0.7858967807869188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:08,198] Trial 22 finished with value: 0.787284409874873 and parameters: {'lambda_l1': 3.960352937811111e-08, 'lambda_l2': 1.4057568372329893e-06, 'num_leaves': 4, 'feature_fraction': 0.9357246023942905, 'bagging_fraction': 0.40596178610994826, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:18,893] Trial 23 finished with value: 0.7774725274725275 and parameters: {'lambda_l1': 5.7489989904472806e-08, 'lambda_l2': 2.7351853538130477e-06, 'num_leaves': 52, 'feature_fraction': 0.9244316258986625, 'bagging_fraction': 0.4274799152106838, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:22,490] Trial 24 finished with value: 0.7777206512425022 and parameters: {'lambda_l1': 1.510722338422316e-05, 'lambda_l2': 1.351710352847491e-05, 'num_leaves': 27, 'feature_fraction': 0.8261421580236766, 'bagging_fraction': 0.4727883941801232, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:24,536] Trial 25 finished with value: 0.7736256435808005 and parameters: {'lambda_l1': 7.235816317230481e-07, 'lambda_l2': 1.7270423784744299e-07, 'num_leaves': 2, 'feature_fraction': 0.8829365296531961, 'bagging_fraction': 0.6415419326897862, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:26,898] Trial 26 finished with value: 0.7776826972445661 and parameters: {'lambda_l1': 9.15879624042074e-08, 'lambda_l2': 0.00015891504706814772, 'num_leaves': 60, 'feature_fraction': 0.6803414676316907, 'bagging_fraction': 0.40562182546197095, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:30,763] Trial 27 finished with value: 0.7777015437392796 and parameters: {'lambda_l1': 2.372132532255354e-06, 'lambda_l2': 4.958622406434735e-08, 'num_leaves': 97, 'feature_fraction': 0.9393873656697741, 'bagging_fraction': 0.5116617348688177, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:53:52,111] Trial 28 finished with value: 0.7812126387702819 and parameters: {'lambda_l1': 3.9757087465532234e-08, 'lambda_l2': 0.000953030057829318, 'num_leaves': 27, 'feature_fraction': 0.5154535249173466, 'bagging_fraction': 0.6847290786270115, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:03,179] Trial 29 finished with value: 0.768729641693811 and parameters: {'lambda_l1': 4.1042074854948574e-07, 'lambda_l2': 5.592690716925183e-05, 'num_leaves': 135, 'feature_fraction': 0.8103905624238568, 'bagging_fraction': 0.5831416786214585, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:12,583] Trial 30 finished with value: 0.7830253251197811 and parameters: {'lambda_l1': 0.0004929567129336858, 'lambda_l2': 9.81739527948266e-07, 'num_leaves': 25, 'feature_fraction': 0.7192296826458278, 'bagging_fraction': 0.7902401186342621, 'bagging_freq': 7}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:13,990] Trial 31 finished with value: 0.7834665759482906 and parameters: {'lambda_l1': 1.085067838847724e-08, 'lambda_l2': 3.803343124583001e-07, 'num_leaves': 4, 'feature_fraction': 0.9749771568454036, 'bagging_fraction': 0.5438232509434774, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:15,577] Trial 32 finished with value: 0.7791274593669802 and parameters: {'lambda_l1': 4.7401210437592986e-08, 'lambda_l2': 4.758076052486901e-06, 'num_leaves': 44, 'feature_fraction': 0.9639744808226376, 'bagging_fraction': 0.5549224204342003, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:17,893] Trial 33 finished with value: 0.7807137954701441 and parameters: {'lambda_l1': 1.1263960095706287e-07, 'lambda_l2': 4.572023994550167e-08, 'num_leaves': 17, 'feature_fraction': 0.9032233148503072, 'bagging_fraction': 0.4545179360159222, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:21,699] Trial 34 finished with value: 0.7838715188792073 and parameters: {'lambda_l1': 3.4485523334446326e-08, 'lambda_l2': 3.053608792721518e-07, 'num_leaves': 38, 'feature_fraction': 0.8566768314432274, 'bagging_fraction': 0.6368511683031488, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:24,620] Trial 35 finished with value: 0.777319587628866 and parameters: {'lambda_l1': 4.069025633005754e-07, 'lambda_l2': 2.392080895736374e-06, 'num_leaves': 35, 'feature_fraction': 0.7944012464664094, 'bagging_fraction': 0.6160290288924299, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:54:50,659] Trial 36 finished with value: 0.772999828679116 and parameters: {'lambda_l1': 2.057167940574201e-06, 'lambda_l2': 1.2751732343197746e-05, 'num_leaves': 251, 'feature_fraction': 0.8519683981830897, 'bagging_fraction': 0.6579493560190817, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:00,343] Trial 37 finished with value: 0.7763567231247839 and parameters: {'lambda_l1': 3.620383786823371e-08, 'lambda_l2': 9.913032959282872e-08, 'num_leaves': 125, 'feature_fraction': 0.8936801783944738, 'bagging_fraction': 0.6312533040961303, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:16,864] Trial 38 finished with value: 0.7807137954701441 and parameters: {'lambda_l1': 1.8158825668556073e-07, 'lambda_l2': 2.684344190433838e-07, 'num_leaves': 19, 'feature_fraction': 0.7904338744159557, 'bagging_fraction': 0.709668496698345, 'bagging_freq': 6}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:21,653] Trial 39 finished with value: 0.7734711455641688 and parameters: {'lambda_l1': 6.319402043477036e-06, 'lambda_l2': 1.4968756941649295e-06, 'num_leaves': 66, 'feature_fraction': 0.7408274020791491, 'bagging_fraction': 0.9297536617644018, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:45,256] Trial 40 finished with value: 0.7747130375192736 and parameters: {'lambda_l1': 0.005365960689160496, 'lambda_l2': 1.2337299013762176e-05, 'num_leaves': 92, 'feature_fraction': 0.9520259519029403, 'bagging_fraction': 0.4510037301959625, 'bagging_freq': 4}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:46,080] Trial 41 finished with value: 0.7832024581768522 and parameters: {'lambda_l1': 2.4877974324292653e-08, 'lambda_l2': 3.439349988050258e-08, 'num_leaves': 11, 'feature_fraction': 0.8979219113210589, 'bagging_fraction': 0.5120448238338666, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:50,132] Trial 42 finished with value: 0.779591135543721 and parameters: {'lambda_l1': 1.0232104118197074e-08, 'lambda_l2': 2.6557917190992804e-07, 'num_leaves': 39, 'feature_fraction': 0.9728770242214974, 'bagging_fraction': 0.5789254892207839, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:55,005] Trial 43 finished with value: 0.7829510441629579 and parameters: {'lambda_l1': 1.435161679401174e-07, 'lambda_l2': 9.462549189025443e-07, 'num_leaves': 16, 'feature_fraction': 0.854800456765239, 'bagging_fraction': 0.6792598284443894, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:55:55,638] Trial 44 finished with value: 0.7841394633395999 and parameters: {'lambda_l1': 0.12543313045066823, 'lambda_l2': 4.3790779209504744e-07, 'num_leaves': 5, 'feature_fraction': 0.944875249235211, 'bagging_fraction': 0.49441878647842785, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:14,629] Trial 45 finished with value: 0.7799280698749785 and parameters: {'lambda_l1': 0.21412386786728915, 'lambda_l2': 5.959501881248335e-06, 'num_leaves': 45, 'feature_fraction': 0.9086966723732053, 'bagging_fraction': 0.4797871130041158, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:16,987] Trial 46 finished with value: 0.7822802197802198 and parameters: {'lambda_l1': 0.14789537668984268, 'lambda_l2': 5.356066171152295e-07, 'num_leaves': 14, 'feature_fraction': 0.9433220451696501, 'bagging_fraction': 0.4384557534416361, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:19,151] Trial 47 finished with value: 0.7807303274472828 and parameters: {'lambda_l1': 0.6822872602994846, 'lambda_l2': 2.691630918716283e-08, 'num_leaves': 35, 'feature_fraction': 0.8691263822547934, 'bagging_fraction': 0.4996719130732066, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:22,787] Trial 48 finished with value: 0.7726081258191351 and parameters: {'lambda_l1': 9.930066497710238, 'lambda_l2': 0.08813639202866776, 'num_leaves': 2, 'feature_fraction': 0.7807366486375357, 'bagging_fraction': 0.6153337580457046, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:40,717] Trial 49 finished with value: 0.7814319972593353 and parameters: {'lambda_l1': 0.014591552940616918, 'lambda_l2': 2.6328197124871476e-05, 'num_leaves': 24, 'feature_fraction': 0.6274655767349525, 'bagging_fraction': 0.5661964183407958, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:45,261] Trial 50 finished with value: 0.7797016972398423 and parameters: {'lambda_l1': 1.1905124833383725, 'lambda_l2': 2.448809155930321e-06, 'num_leaves': 86, 'feature_fraction': 0.8426352157689012, 'bagging_fraction': 0.5341862846318628, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:46,548] Trial 51 finished with value: 0.7830107895187531 and parameters: {'lambda_l1': 0.04643585836882674, 'lambda_l2': 3.9190781107282135e-07, 'num_leaves': 12, 'feature_fraction': 0.9629670727992216, 'bagging_fraction': 0.4901275615113315, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:56:50,307] Trial 52 finished with value: 0.7746409113422487 and parameters: {'lambda_l1': 2.7765895669398976e-08, 'lambda_l2': 1.0362877233192058e-07, 'num_leaves': 2, 'feature_fraction': 0.9951152484551505, 'bagging_fraction': 0.5986525439963031, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:07,939] Trial 53 finished with value: 0.781287348471914 and parameters: {'lambda_l1': 3.407425315892697e-07, 'lambda_l2': 5.522819821402037e-07, 'num_leaves': 31, 'feature_fraction': 0.9195162921280116, 'bagging_fraction': 0.5369603595518273, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:13,166] Trial 54 finished with value: 0.771438332763922 and parameters: {'lambda_l1': 1.0967461471870433e-06, 'lambda_l2': 1.936914854810931e-07, 'num_leaves': 57, 'feature_fraction': 0.9401374898876508, 'bagging_fraction': 0.405528263308202, 'bagging_freq': 5}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:15,108] Trial 55 finished with value: 0.7811965811965812 and parameters: {'lambda_l1': 7.338335864188616e-08, 'lambda_l2': 1.9715286008052378e-08, 'num_leaves': 13, 'feature_fraction': 0.8793023095800804, 'bagging_fraction': 0.5955999322743902, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:20,603] Trial 56 finished with value: 0.7800376004101863 and parameters: {'lambda_l1': 2.0407032485641085e-08, 'lambda_l2': 1.740604243352818e-06, 'num_leaves': 47, 'feature_fraction': 0.9731699092842767, 'bagging_fraction': 0.6353888165185937, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:37,901] Trial 57 finished with value: 0.7810529902879536 and parameters: {'lambda_l1': 0.00011533868589507492, 'lambda_l2': 7.05039227953608e-08, 'num_leaves': 21, 'feature_fraction': 0.9295988808032378, 'bagging_fraction': 0.5264580970761545, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:57:47,693] Trial 58 finished with value: 0.7685201026518392 and parameters: {'lambda_l1': 0.0013323821453342457, 'lambda_l2': 7.0677368740100634e-06, 'num_leaves': 209, 'feature_fraction': 0.958207546615947, 'bagging_fraction': 0.46817612052522484, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:03,323] Trial 59 finished with value: 0.7834699453551912 and parameters: {'lambda_l1': 9.558463489564048e-06, 'lambda_l2': 6.442097801879512e-07, 'num_leaves': 10, 'feature_fraction': 0.8821215292446293, 'bagging_fraction': 0.7633005907229778, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:09,605] Trial 60 finished with value: 0.7764220656470184 and parameters: {'lambda_l1': 1.716185708641245e-05, 'lambda_l2': 4.292063960943329e-05, 'num_leaves': 69, 'feature_fraction': 0.7491893703268683, 'bagging_fraction': 0.7417589611077269, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:10,379] Trial 61 finished with value: 0.7800136425648021 and parameters: {'lambda_l1': 4.810450062080489e-06, 'lambda_l2': 6.979025019244418e-07, 'num_leaves': 9, 'feature_fraction': 0.8868014862321877, 'bagging_fraction': 0.7726178118316527, 'bagging_freq': 4}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:14,911] Trial 62 finished with value: 0.7729676140118968 and parameters: {'lambda_l1': 9.170013023509763e-07, 'lambda_l2': 3.1709406736984574e-07, 'num_leaves': 2, 'feature_fraction': 0.9988752373293969, 'bagging_fraction': 0.8657996439112674, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:33,334] Trial 63 finished with value: 0.781565872879904 and parameters: {'lambda_l1': 2.441797880428963e-07, 'lambda_l2': 3.728422235842384e-06, 'num_leaves': 21, 'feature_fraction': 0.910144947458077, 'bagging_fraction': 0.8341408923852035, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:37,001] Trial 64 finished with value: 0.7772073921971252 and parameters: {'lambda_l1': 3.215875505095308, 'lambda_l2': 1.5200112685014277e-07, 'num_leaves': 31, 'feature_fraction': 0.8134175609244937, 'bagging_fraction': 0.5674968121548711, 'bagging_freq': 4}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:58:38,443] Trial 65 finished with value: 0.7846759021720541 and parameters: {'lambda_l1': 9.363181055180053e-08, 'lambda_l2': 1.0159156091105864e-06, 'num_leaves': 10, 'feature_fraction': 0.8346948856313913, 'bagging_fraction': 0.6963565980261525, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:01,821] Trial 66 finished with value: 0.7772641670946756 and parameters: {'lambda_l1': 4.991548605061184e-07, 'lambda_l2': 1.2123024571271474e-06, 'num_leaves': 42, 'feature_fraction': 0.8476029633525047, 'bagging_fraction': 0.7673315552495882, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:04,507] Trial 67 finished with value: 0.780229177355909 and parameters: {'lambda_l1': 3.045973816934975e-06, 'lambda_l2': 0.00010588914104671332, 'num_leaves': 25, 'feature_fraction': 0.8286303159316852, 'bagging_fraction': 0.6945260561926897, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:05,602] Trial 68 finished with value: 0.7860425531914895 and parameters: {'lambda_l1': 8.403274807092867e-08, 'lambda_l2': 1.0330237340696673e-05, 'num_leaves': 10, 'feature_fraction': 0.8717044254272769, 'bagging_fraction': 0.6533274447536646, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:29,554] Trial 69 finished with value: 0.7793008910212474 and parameters: {'lambda_l1': 1.2769746483152483e-07, 'lambda_l2': 0.00037382240276440933, 'num_leaves': 37, 'feature_fraction': 0.8648380208337934, 'bagging_fraction': 0.6656125381695668, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:40,418] Trial 70 finished with value: 0.7748798901853123 and parameters: {'lambda_l1': 6.36592945325734e-08, 'lambda_l2': 8.744631912674759e-06, 'num_leaves': 110, 'feature_fraction': 0.8029748452384582, 'bagging_fraction': 0.7161913738074619, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-17 23:59:58,327] Trial 71 finished with value: 0.781855249745158 and parameters: {'lambda_l1': 4.2636653643158117e-05, 'lambda_l2': 2.727187908235324e-06, 'num_leaves': 9, 'feature_fraction': 0.8382473895607108, 'bagging_fraction': 0.6476991112724213, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:02,166] Trial 72 finished with value: 0.784313725490196 and parameters: {'lambda_l1': 1.5510271791247822e-06, 'lambda_l2': 2.1573418140866965e-05, 'num_leaves': 19, 'feature_fraction': 0.8762423738738191, 'bagging_fraction': 0.6745484288231527, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:09,002] Trial 73 finished with value: 0.78259385665529 and parameters: {'lambda_l1': 2.357695084233657e-07, 'lambda_l2': 1.9973364140423042e-05, 'num_leaves': 17, 'feature_fraction': 0.7640220656877096, 'bagging_fraction': 0.676150650594716, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:16,554] Trial 74 finished with value: 0.7763857902865968 and parameters: {'lambda_l1': 1.3322915280836993e-06, 'lambda_l2': 1.9000263278941174e-05, 'num_leaves': 28, 'feature_fraction': 0.9080959869789295, 'bagging_fraction': 0.6986053295729224, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:27,971] Trial 75 finished with value: 0.7839972527472526 and parameters: {'lambda_l1': 7.328990775989937e-08, 'lambda_l2': 4.36203270350139e-06, 'num_leaves': 51, 'feature_fraction': 0.8665156708225793, 'bagging_fraction': 0.6231015535207266, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:53,223] Trial 76 finished with value: 0.7751458976999658 and parameters: {'lambda_l1': 8.460054767093618e-08, 'lambda_l2': 4.959798723407663e-06, 'num_leaves': 51, 'feature_fraction': 0.9194369785009986, 'bagging_fraction': 0.6185807962997604, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:00:59,231] Trial 77 finished with value: 0.780962128966223 and parameters: {'lambda_l1': 6.081467759846068e-07, 'lambda_l2': 7.642385659625162e-05, 'num_leaves': 21, 'feature_fraction': 0.9425208346199326, 'bagging_fraction': 0.6607480787667908, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:01:20,362] Trial 78 finished with value: 0.7795505232458397 and parameters: {'lambda_l1': 1.7298150270085262e-08, 'lambda_l2': 9.763163386695621e-06, 'num_leaves': 61, 'feature_fraction': 0.8930351284564287, 'bagging_fraction': 0.6240619552306716, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:01:51,085] Trial 79 finished with value: 0.7754193769257104 and parameters: {'lambda_l1': 5.0702365914337676e-08, 'lambda_l2': 0.0001779348970508537, 'num_leaves': 161, 'feature_fraction': 0.5196193637553858, 'bagging_fraction': 0.5973896974824586, 'bagging_freq': 1}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:01:54,971] Trial 80 finished with value: 0.7860560492139439 and parameters: {'lambda_l1': 1.461264896104747e-06, 'lambda_l2': 1.6232155972861276e-06, 'num_leaves': 8, 'feature_fraction': 0.8721507382128245, 'bagging_fraction': 0.6482340667029309, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:01:59,148] Trial 81 finished with value: 0.7854334074200718 and parameters: {'lambda_l1': 1.881894943594622e-06, 'lambda_l2': 1.689874702559035e-06, 'num_leaves': 7, 'feature_fraction': 0.8756604398298689, 'bagging_fraction': 0.6490714602100769, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:00,004] Trial 82 finished with value: 0.7862868838478594 and parameters: {'lambda_l1': 1.7256798044689291e-06, 'lambda_l2': 1.163771621826e-06, 'num_leaves': 9, 'feature_fraction': 0.8353073998326047, 'bagging_fraction': 0.6542939107047974, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:17,659] Trial 83 finished with value: 0.7817622950819673 and parameters: {'lambda_l1': 1.7994176197387831e-06, 'lambda_l2': 1.7736744844850833e-06, 'num_leaves': 17, 'feature_fraction': 0.8211148234321233, 'bagging_fraction': 0.729883952653518, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:20,372] Trial 84 finished with value: 0.7848101265822786 and parameters: {'lambda_l1': 3.6006220836911997e-06, 'lambda_l2': 1.0770952907987507e-06, 'num_leaves': 8, 'feature_fraction': 0.8351727987080324, 'bagging_fraction': 0.6479108565263029, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:22,389] Trial 85 finished with value: 0.7842869342442358 and parameters: {'lambda_l1': 3.925860448307523e-06, 'lambda_l2': 9.966456338218722e-07, 'num_leaves': 8, 'feature_fraction': 0.7132611984228483, 'bagging_fraction': 0.6533610314603553, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:45,260] Trial 86 finished with value: 0.7789799072642968 and parameters: {'lambda_l1': 1.8075806177241167e-05, 'lambda_l2': 3.139514428192302e-06, 'num_leaves': 30, 'feature_fraction': 0.8312801672283558, 'bagging_fraction': 0.6848049889515938, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:50,410] Trial 87 finished with value: 0.7806551191905334 and parameters: {'lambda_l1': 9.804970287361108e-06, 'lambda_l2': 1.7930369993059429e-06, 'num_leaves': 12, 'feature_fraction': 0.8090407221991502, 'bagging_fraction': 0.6441763453716542, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:02:51,286] Trial 88 finished with value: 0.7850467289719626 and parameters: {'lambda_l1': 3.508020252537404e-07, 'lambda_l2': 8.054751956898838e-07, 'num_leaves': 8, 'feature_fraction': 0.7898856368892591, 'bagging_fraction': 0.7078363252616564, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:10,119] Trial 89 finished with value: 0.7843137254901961 and parameters: {'lambda_l1': 3.193242342349758e-07, 'lambda_l2': 6.044987305034324, 'num_leaves': 32, 'feature_fraction': 0.7878059292684477, 'bagging_fraction': 0.585294384366157, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:11,120] Trial 90 finished with value: 0.7672600232906339 and parameters: {'lambda_l1': 7.674287786388874e-07, 'lambda_l2': 1.932540223290328e-07, 'num_leaves': 2, 'feature_fraction': 0.6601552604806119, 'bagging_fraction': 0.7089774849644522, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:13,447] Trial 91 finished with value: 0.7816997943797123 and parameters: {'lambda_l1': 1.7144774830506908e-07, 'lambda_l2': 9.185902065222506e-07, 'num_leaves': 14, 'feature_fraction': 0.8440636208016413, 'bagging_fraction': 0.6905398637856978, 'bagging_freq': 3}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:15,377] Trial 92 finished with value: 0.7824303537856777 and parameters: {'lambda_l1': 2.837102986689292e-06, 'lambda_l2': 1.384929464758262e-06, 'num_leaves': 8, 'feature_fraction': 0.7709323825630732, 'bagging_fraction': 0.6647318685194161, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:18,552] Trial 93 finished with value: 0.7825196312734721 and parameters: {'lambda_l1': 5.886008538681126e-07, 'lambda_l2': 2.2830355996423776e-06, 'num_leaves': 26, 'feature_fraction': 0.8588624005690707, 'bagging_fraction': 0.7033269506674107, 'bagging_freq': 7}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:23,178] Trial 94 finished with value: 0.7836818649297225 and parameters: {'lambda_l1': 1.1052071059087303e-06, 'lambda_l2': 7.164810940563319e-07, 'num_leaves': 22, 'feature_fraction': 0.7970231576341632, 'bagging_fraction': 0.7234784610506606, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:24,515] Trial 95 finished with value: 0.7850212765957446 and parameters: {'lambda_l1': 1.351032476593488e-07, 'lambda_l2': 5.809459613190575e-06, 'num_leaves': 7, 'feature_fraction': 0.893624918556124, 'bagging_fraction': 0.6085451338371857, 'bagging_freq': 2}. Best is trial 22 with value: 0.787284409874873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:42,138] Trial 96 finished with value: 0.7878685191460522 and parameters: {'lambda_l1': 3.4482046572642613e-07, 'lambda_l2': 6.557494099284118e-06, 'num_leaves': 6, 'feature_fraction': 0.9298593243367005, 'bagging_fraction': 0.6392974057353847, 'bagging_freq': 3}. Best is trial 96 with value: 0.7878685191460522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:51,123] Trial 97 finished with value: 0.7825046984452417 and parameters: {'lambda_l1': 2.8480368946696534e-07, 'lambda_l2': 3.530254891812464e-05, 'num_leaves': 16, 'feature_fraction': 0.9287358842540062, 'bagging_fraction': 0.6111257927280047, 'bagging_freq': 3}. Best is trial 96 with value: 0.7878685191460522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:03:52,484] Trial 98 finished with value: 0.7718775847808106 and parameters: {'lambda_l1': 1.2663829013467895e-07, 'lambda_l2': 6.494007876133363e-06, 'num_leaves': 2, 'feature_fraction': 0.8985551033448742, 'bagging_fraction': 0.5595841743985195, 'bagging_freq': 3}. Best is trial 96 with value: 0.7878685191460522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 11481, number of negative: 9307\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 430\n",
      "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552290 -> initscore=0.209927\n",
      "[LightGBM] [Info] Start training from score 0.209927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 00:04:46,841] Trial 99 finished with value: 0.7755597986460685 and parameters: {'lambda_l1': 3.962081591055929e-07, 'lambda_l2': 1.147039870372715e-05, 'num_leaves': 190, 'feature_fraction': 0.9177125666377189, 'bagging_fraction': 0.6326337674136286, 'bagging_freq': 2}. Best is trial 96 with value: 0.7878685191460522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 100\n",
      "Best trial: {'lambda_l1': 3.4482046572642613e-07, 'lambda_l2': 6.557494099284118e-06, 'num_leaves': 6, 'feature_fraction': 0.9298593243367005, 'bagging_fraction': 0.6392974057353847, 'bagging_freq': 3}\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'binary_logloss',\n",
    "#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "#         # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "#     }\n",
    "\n",
    "#     gbm = lgb.train(param, train_data)\n",
    "#     preds = gbm.predict(x_test)\n",
    "#     pred_labels = np.rint(preds)\n",
    "#     accuracy = f1_score(y_test, pred_labels)\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = [i.get('model') for i in model_result]\n",
    "# results = [i.get('result') for i in model_result]\n",
    "\n",
    "\n",
    "# accuracy = [accuracy_score(y_test, i) for i in results]\n",
    "# precision = [precision_score(y_test, i) for i in results]\n",
    "# recall = [recall_score(y_test, i) for i in results]\n",
    "# f1 = [f1_score(y_test, i) for i in results]\n",
    "# auc = [roc_auc_score(y_test, i) for i in results]\n",
    "\n",
    "# table = pd.DataFrame({\n",
    "#     'name': name,\n",
    "#     'f1': f1,\n",
    "#     'accuracy': accuracy,\n",
    "#     'precision': precision,\n",
    "#     'recall': recall,\n",
    "#     'auc': auc\n",
    "# })\n",
    "\n",
    "# # sort by accuracy\n",
    "# table = table.sort_values(by='f1', ascending=False)\n",
    "\n",
    "# print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "# import joblib\n",
    "\n",
    "# df = pd.read_csv('./datasets/mushroom.csv')\n",
    "# df2 = pd.read_csv('./datasets/mushrooms.csv')\n",
    "\n",
    "# # rename columns\n",
    "# df2.rename(columns={'bruises':'ruises'}, inplace=True)\n",
    "\n",
    "# # df = pd.concat([df, df2], axis=0)\n",
    "# # df = pd.concat([df, df2], axis=0)\n",
    "\n",
    "# df.drop(columns = ['veil-type'], inplace = True)\n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "# mask = df.isnull()\n",
    "# # class 使用 OneHotEncoder\n",
    "# df['class'] = OneHotEncoder().fit_transform(df['class'].values.reshape(-1, 1)).toarray()\n",
    "# # 其余使用 LabelEncoder 编码\n",
    "# for col in df.columns:\n",
    "#     if col != 'class':\n",
    "#         df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# df.where(~mask, np.nan, inplace=True)\n",
    "\n",
    "# # KNN Imputer\n",
    "# df = pd.DataFrame(KNNImputer(\n",
    "#     n_neighbors=5).fit_transform(df), columns=df.columns)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# x = df.drop('class', axis=1)\n",
    "# y = df['class']\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "# print(x_train.shape, y_train.shape)\n",
    "# print(x_test.shape, y_test.shape)\n",
    "\n",
    "# name = []\n",
    "# accuracy = []\n",
    "# precision = []\n",
    "# recall = []\n",
    "# f1 = []\n",
    "# auc = []\n",
    "\n",
    "# for path in pathlib.Path('./results').glob('*.pkl'):\n",
    "#     try:\n",
    "#         model = joblib.load(path)\n",
    "#         pred = model.predict(x_test)\n",
    "        \n",
    "#         name.append(path.stem)\n",
    "#         accuracy.append(accuracy_score(y_test, pred))\n",
    "#         precision.append(precision_score(y_test, pred))\n",
    "#         recall.append(recall_score(y_test, pred))\n",
    "#         f1.append(f1_score(y_test, pred))\n",
    "#         auc.append(roc_auc_score(y_test, pred))\n",
    "#     except Exception as e:\n",
    "#         print(path)\n",
    "\n",
    "# print(pd.DataFrame({\n",
    "#     'name': name,\n",
    "#     'f1': f1,\n",
    "#     'accuracy': accuracy,\n",
    "#     'precision': precision,\n",
    "#     'recall': recall,\n",
    "#     'auc': auc\n",
    "# }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
